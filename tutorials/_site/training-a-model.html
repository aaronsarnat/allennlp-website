<h3 id="training-a-model">Training a Model</h3>

<p>In this tutorial we’ll train a simple part-of-speech tagger using AllenNLP.
The model is defined in <a href="https://github.com/allenai/allennlp/blob/master/allennlp/models/simple_tagger.py">allennlp/models/simple_tagger.py</a>.
It consists of a word embedding layer followed by an LSTM.</p>

<p>Our dataset will be a subset of the <a href="http://www.nltk.org/nltk_data/">Brown Corpus</a>.
In particular, we will train a model on 4000 randomly chosen sentences (<code class="highlighter-rouge">sentences.small.train</code>) and use a different ~1000 randomly chosen sentences
as the validation set (<code class="highlighter-rouge">sentences.small.dev</code>).</p>

<p>One of the key design principles behind AllenNLP is that
you configure experiments using JSON files. (More specifically, <a href="https://github.com/typesafehub/config/blob/master/HOCON.md">HOCON</a> files.)</p>

<p>Our tagging experiment is defined in
<a href="https://github.com/allenai/allennlp/blob/master/tutorials/getting_started/simple_tagger.json">tutorials/getting_started/simple_tagger.json</a>.
You can peek at it now if you want; we’ll go through it in detail in the next tutorial.
Right at this instant you might care about the <code class="highlighter-rouge">trainer</code> section, which specifies how we want to train our model:</p>

<div class="language-js highlighter-rouge"><pre class="highlight"><code>  <span class="s2">"trainer"</span><span class="err">:</span> <span class="p">{</span>
    <span class="s2">"num_epochs"</span><span class="err">:</span> <span class="mi">40</span><span class="p">,</span>
    <span class="s2">"patience"</span><span class="err">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="s2">"cuda_device"</span><span class="err">:</span> <span class="o">-</span><span class="mi">1</span>
  <span class="p">}</span>
</code></pre>
</div>

<p>Here the <code class="highlighter-rouge">num_epochs</code> parameter specifies that we want to make 40 training passes through the training dataset.
On a recent Macbook each epoch of this model on this dataset takes about a minute,
so this training should take about 40, unless it stops early. <code class="highlighter-rouge">patience</code>
controls the early stopping – if our validation metric doesn’t improve for
this many epochs, training halts. And if you have a GPU you can change <code class="highlighter-rouge">cuda_device</code> to 0 to use it.</p>

<p>Change any of those if you want to, and then run</p>

<div class="highlighter-rouge"><pre class="highlight"><code>$ python -m allennlp.run train tutorials/getting_started/simple_tagger.json --serialization_dir /tmp/tutorials/getting_started
</code></pre>
</div>

<p>The <code class="highlighter-rouge">serialization_dir</code> argument specifies the directory where the model’s vocabulary and checkpointed weights will be saved.</p>

<p>This command will download the datasets and cache them locally,
log all of the parameters it’s using,
and then display the progress and results of each epoch:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>2017-08-23 18:07:14,700 - INFO - allennlp.training.trainer - Epoch 2/40
accuracy: 0.51, loss: 2.06, accuracy3: 0.67 ||: 100%|##########| 125/125 [01:08&lt;00:00,  2.03it/s]
accuracy: 0.61, loss: 1.65, accuracy3: 0.75 ||: 100%|##########| 32/32 [00:06&lt;00:00,  4.96it/s]
2017-08-23 18:08:29,397 - INFO - allennlp.training.trainer - Training accuracy : 0.506099    Validation accuracy : 0.606811
2017-08-23 18:08:29,398 - INFO - allennlp.training.trainer - Training loss : 2.061412    Validation loss : 1.646712
2017-08-23 18:08:29,398 - INFO - allennlp.training.trainer - Training accuracy3 : 0.672000    Validation accuracy3 : 0.753761
2017-08-23 18:08:29,423 - INFO - allennlp.training.trainer - Best validation performance so far. Copying weights to /tmp/tutorials/getting_started/best.th'.
</code></pre>
</div>

<p>Here <code class="highlighter-rouge">accuracy</code> measures how often our model predicted the “correct” part of speech tag as most probable,
while <code class="highlighter-rouge">accuracy3</code> measures how often the correct tag was one of the <em>three</em> most probable.
<code class="highlighter-rouge">loss</code> measures <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross entropy</a>
 and is the objective being used to train the model. You want to make sure
 it’s mostly decreasing during training.</p>

<p>After 30 epochs the performance on the validation set seems to top out:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>2017-08-23 18:40:46,632 - INFO - allennlp.training.trainer - Epoch 30/40
accuracy: 0.97, loss: 0.10, accuracy3: 1.00 ||: 100%|##########| 125/125 [01:04&lt;00:00,  1.86it/s]
accuracy: 0.92, loss: 0.40, accuracy3: 0.97 ||: 100%|##########| 32/32 [00:05&lt;00:00,  5.58it/s]
2017-08-23 18:41:57,236 - INFO - allennlp.training.trainer - Training accuracy : 0.966363    Validation accuracy : 0.916993
2017-08-23 18:41:57,236 - INFO - allennlp.training.trainer - Training loss : 0.098178    Validation loss : 0.401380
2017-08-23 18:41:57,237 - INFO - allennlp.training.trainer - Training accuracy3 : 0.995176    Validation accuracy3 : 0.973490
</code></pre>
</div>

<p>This means that 92% of the time our model predicted the correct tag on the validation dataset,
and 97% of the time the correct tag was in the model’s “top 3”.
Not ground-breaking performance, but this is a pretty simple model, and
if you look at the data there’s a lot of different tags!</p>

<p>Now that the model is trained, there should be a bunch of files in the serialization directory. The <code class="highlighter-rouge">vocabulary</code> directory
contains the model’s vocabularies, each of which is a (distinct) encoding of strings as integers.
In our case, we’ll have one for <code class="highlighter-rouge">tokens</code> (i.e. words) and another for <code class="highlighter-rouge">tags</code>. The various
<code class="highlighter-rouge">training_state_epoch_XX.th</code> files contain the state of the trainer after each epoch (<code class="highlighter-rouge">.th</code> is the suffix for serialized torch tensors),
so that you could resume training where you left off, if you wanted to.
Similarly, the <code class="highlighter-rouge">model_state_epoch_XX.th</code> files contain the model weights after each epoch.
<code class="highlighter-rouge">best.th</code> contains the <em>best</em> weights (that is, those from the epoch with the smallest <code class="highlighter-rouge">loss</code> on the validation dataset).</p>

<p>Finally, there is an “archive” file <code class="highlighter-rouge">model.tar.gz</code> that contains the training configuration,
the <code class="highlighter-rouge">best</code> weights, and the <code class="highlighter-rouge">vocabulary</code>.</p>

<h3 id="evaluating-a-model">Evaluating a Model</h3>

<p>Once you’ve trained a model, you likely want to evaluate it on another dataset.
We have another 1000 sentences in the file <code class="highlighter-rouge">sentences.small.test</code>, which
is shared publicly on Amazon S3.</p>

<p>We can use the <code class="highlighter-rouge">evaluate</code> command, giving it the archived model and the evaluation dataset:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>$ python -m allennlp.run evaluate --archive_file /tmp/tutorials/getting_started/model.tar.gz --evaluation_data_file https://allennlp.s3.amazonaws.com/datasets/getting-started/sentences.small.test
</code></pre>
</div>

<p>When you run this it will load the archived model, download and cache the evaluation dataset, and then make predictions:</p>

<div class="highlighter-rouge"><pre class="highlight"><code>2017-08-23 19:49:18,451 - INFO - allennlp.models.archival - extracting archive file /tmp/tutorials/getting_started/model.tar.gz to temp dir /var/folders/_n/mdsjzvcs6s705kpn87f399880000gp/T/tmptgu44ulc
2017-08-23 19:49:18,643 - INFO - allennlp.commands.evaluate - Reading evaluation data from https://allennlp.s3.amazonaws.com/datasets/getting-started/sentences.small.test
2017-08-23 19:49:18,643 - INFO - allennlp.common.file_utils - https://allennlp.s3.amazonaws.com/datasets/getting-started/sentences.small.test not found in cache, downloading to /Users/joelg/.allennlp/datasets/aHR0cHM6Ly9hbGxlbm5scC5zMy5hbWF6b25hd3MuY29tL2RhdGFzZXRzL2dldHRpbmctc3RhcnRlZC9zZW50ZW5jZXMuc21hbGwudGVzdA==
100%|████████████████████████████████████████████████████████████████████████████████████| 170391/170391 [00:00&lt;00:00, 1306579.69B/s]
2017-08-23 19:49:20,203 - INFO - allennlp.data.dataset_readers.sequence_tagging - Reading instances from lines in file at: /Users/joelg/.allennlp/datasets/aHR0cHM6Ly9hbGxlbm5scC5zMy5hbWF6b25hd3MuY29tL2RhdGFzZXRzL2dldHRpbmctc3RhcnRlZC9zZW50ZW5jZXMuc21hbGwudGVzdA==
1000it [00:00, 36100.84it/s]
2017-08-23 19:49:20,233 - INFO - allennlp.data.dataset - Indexing dataset
100%|██████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00&lt;00:00, 7155.68it/s]
2017-08-23 19:49:20,373 - INFO - allennlp.commands.evaluate - Iterating over dataset
100%|████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:05&lt;00:00,  5.47it/s]
2017-08-23 19:49:26,228 - INFO - allennlp.commands.evaluate - Finished evaluating.
2017-08-23 19:49:26,228 - INFO - allennlp.commands.evaluate - Metrics:
2017-08-23 19:49:26,228 - INFO - allennlp.commands.evaluate - accuracy: 0.9070572302753674
2017-08-23 19:49:26,228 - INFO - allennlp.commands.evaluate - accuracy3: 0.9681496714651151
</code></pre>
</div>

<p>There is also a command line option to use a GPU, if you have one.</p>

<h3 id="making-predictions">Making Predictions</h3>

<p>Finally, what’s the good of training a model if you can’t use it to make predictions?
The <code class="highlighter-rouge">predict</code> command takes an archived model and a <a href="https://en.wikipedia.org/wiki/JSON_Streaming#Line_delimited_JSON">JSON lines</a>
file of inputs and makes predictions using the model.</p>

<p>Here, the “predictor” for the tagging model expects a JSON blob containing a sentence:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>cat <span class="sh">&lt;&lt;EOF &gt;&gt; inputs.txt
{"sentence": "I am reading a tutorial."}
{"sentence": "Natural language processing is easy."}
EOF
</span></code></pre>
</div>

<p>After which we can make predictions:</p>

<div class="language-bash highlighter-rouge"><pre class="highlight"><code><span class="gp">$ </span>python -m allennlp.run predict /tmp/tutorials/getting_started/model.tar.gz inputs.txt
... lots of logging omitted
<span class="o">{</span><span class="s2">"tags"</span>: <span class="o">[</span><span class="s2">"ppss"</span>, <span class="s2">"bem"</span>, <span class="s2">"vbg"</span>, <span class="s2">"at"</span>, <span class="s2">"nn"</span>, <span class="s2">"."</span><span class="o">]</span>, <span class="s2">"class_probabilities"</span>: <span class="o">[[</span> ... <span class="o">]]}</span>
<span class="o">{</span><span class="s2">"tags"</span>: <span class="o">[</span><span class="s2">"jj"</span>, <span class="s2">"nn"</span>, <span class="s2">"nn"</span>, <span class="s2">"bez"</span>, <span class="s2">"jj"</span>, <span class="s2">"."</span><span class="o">]</span>, <span class="s2">"class_probabilities"</span>: <span class="o">[[</span> ... <span class="o">]]}</span>
</code></pre>
</div>

<p>Here the <code class="highlighter-rouge">"tags"</code> are the part-of-speech tags for each sentence, and the
<code class="highlighter-rouge">"class_probabilities"</code> are the predicted distributions of tags for each sentence
(and are not shown above, as there are a lot of them).</p>

<h3 id="next-steps">Next Steps</h3>

<p>Continue on to our <a href="configuration">Configuration</a> tutorial.</p>
